\begin{center}
 {\large \bf ABSTRACT } \\ \vspace{0.2in}
 {\large \bf Improve System Performance for Next Generation Networks} \\ \vspace{0.2in}

 {\large \bf by } \\ \vspace{0.3in}
 {\large \bf Tingting Lu} \\ \vspace{0.3in}
 {\large \bf Advisor: Shivendra S. Panwar}  \\ \vspace{0.2in}

 {\bf Submitted in Partial Fulfillment of the Requirements for\\
 the Degree of Doctor of Philosophy (Electrical Engineering)
 } \\
 \vspace{0.2in}
 {\bf May 2017} \\
 \vspace{0.2in}

\end{center}

\par Due to the severe shortage in conventional cellular bands, millimeter wave (mmWave) frequency spectrum, between 30 and 300 GHz, has been considered as a key component to address bandwidth deficiency in the next generation network. mmWave has small wavelengths which result in high path loss and sensitivity to obstructions. Obstructions caused large-scale shadow fading. Shadow fading can bring in significant received power loss for a wide area. In reality, shadow fading at two different positions are correlated to each other. Correlated shadow fading downgrade system performance by introducing high outage probability and long outage duration, which will lead to connection drop. To fully utilize the bandwidth of wireless links, shadow fading needs to be mitigated.
\par This thesis focuses on studying the system performance in terms of outage probability and outage duration under correlated shadow fading, developing efficient ways to mitigate shadow fading, reduce outage and provide better Quality of Service (Qos) to users. Starting from the downlink of a single-cell model under distance-angle correlated shadow fading, channel variations due to correlated shadow fading is studied. A cooprative communication solution to reduce the frequency and duration of dropped connections by employing relays are proposed. Secondly, to simplify the analysis, the most commonly used shadowing correlation model: exponential correlation model, are introduced to investigate the single-cell system. We attempt to characterize shadow fading so as to accurately study its impact on the application layer QoS. A first-order Markov chain model is developed and validated. Using the proposed Markov chain model, the frequency and duration of outage near the edge of a singel cell can be analyzed. This model provides an efficient way to describe the channel variations. Thirdly, the research is expanded to multi-cell model. Simulations are run to explore the outage probability and outage duration given different Base Station (BS) densities and layouts. Increasing BS density is proved to be a valid way to reduce outge probability in some particular case. Meanwhile, it is an efficient way to reduce outage durations, which has significant benefit for upper layer applications. In the end, after exploring pyhsical layer, we continue to transport layer. As a result of the characteristics of mmWave channel, the legacy Transmission Control Protocol (TCP) may not work well for next generation network. To verify this, the performance of exsiting TCP on a mimic mmWave channel with periodic on-off behavior are investigated. The result shows that exisiting TCP congestion control algorithms are not optimal for the testing channel. To develop a new proper TCP congestion control protocol for next generation networks, reduce congestion detection time is a key component. Therefore, we propose a fast end user congestion detection scheme which provides high precision when network topology does not change fast.

