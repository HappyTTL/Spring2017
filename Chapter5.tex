\chapter{Transport Layer Protocols for Next Generation Networks}\label{ch:5} 


\par The rapid increase of smartphone use generates dramatical demand increase for capacity in mobile broadband communications every year. The wireless carriers and researchers are motivated to explore the underutilized millimeter wave (mmWave) frequency spectrum for next generation (Fifth Generation) broadband cellular communication networks. Fifth generation (5G) mmWave communication system will provide high speed, high capacity wireless communication networks to users. Meanwhile, mmWave channel is sensitive to obstructions, which results in frequent capacity fluctuations. Legacy transmission control protocols (TCP) might not work properly with the new channel; therefore, we first investigate the performance of legacy TCP on mmWave channel. Results indicate that mmWave channel requires a fast congestion detection scheme and an aggressive recovery scheme. Therefore, this chapter focuses on fast congestion detection scheme and presents a new end-to-end TCP congestion detection scheme for mmWave communication networks: Virtual Explicit Congestion Notification (Virtual ECN). Virtual ECN leads to the flouring of the fast reacting congestion control protocol design.
\section{Introduction}
\par  mmWave channel provides a tremendous amount of capacity and relatively low delay while suffering from high propagation loss and sensitivity to blockage. Simulations and measurements revealed that the capacity gain is significant, compared to current cellular systems \cite{akdeniz2014millimeter,bai2015coverage}. The last hop wireless channel capacity is no longer a bottleneck problem; meanwhile, mmWave channels are prone to variations due to the blockage from walls, trees or even human body \cite{lu2012modeling, zhao201328, alejos2008measurement}. This feature brings frequent capacity variations to the channel. Typically, the channel switches periodically between Line-of-Sight (LOS) and Non-Line-of-Sight (NLOS). The upper layer channel provided by mmWave communication network is not yet standardized. But in general, there is a consensus that mmWave channel will provide high capacity and low latency with frequent capacity fluctuations. For mmWave wireless communication networks, the legacy TCP congestion control might not work well due to the slow congestion detection and conservative loss recovery. Our investigation on legacy TCP with emulated mmWave channel confirms this. Therefore, designing a new TCP congestion control scheme without involving any intermediate network device is necessary. In this chapter, we will illustrate the initial step to design such a scheme: develop an end-to-end fast data-driven congestion detection algorithm.
\par In recent years, several promising TCP congestion control protocols have been developed such as BIC TCP \cite{xu2004binary}, CUBIC TCP \cite{ha2008cubic}, Compound TCP \cite{tan2006compound} and so on. Most of these TCP congestion control schemes rely on three duplicate ACKs or Retransmission Timeout to detect the congestion and packet loss. This means TCP needs to wait for at least three Round-Trip-Time (RTT) or a retransmission timeout before entering into recovery mode or slow start mode. The slow congestion detection will deteriorate the network performance for a high capacity, low latency mmWave channel with frequent capacity variations. First, more congestion and packet loss might occur during this interval and cause high payoff to recover from the loss for a high capacity link. Secondly, for a network with capacity variations, the slow congestion detection scheme might provide out of date information of the congestion status, which makes the TCP congestion control protocol extremely conservative.
\par This chapter is organized as follows: Section \ref{Related Work and Motivations} explains the related work on TCP congestion control and mmWave channel. Motivations to design this scheme is demonstrated in this section. In section \ref{legacy}, we investigate the legacy TCP congestion control performance on anemulated emulated mmWave channel for both NewReno and Cubic. Section \ref{Design} demonstrates the proposed fast end-to-end congestion detection scheme in detail including network simulation setup, data process and congestion prediction. Section \ref{Conclusions and Future Work} summarizes the chapter and proposes future work, which can be accomplished based on this scheme.

\section{Related Work}
\label{Related Work and Motivations}
In this section, we present the related work on mmWave channel modeling and data-driven TCP congestion control.  
\subsection{mmWave Channel}
\par In \cite{niu2015survey}\cite{rappaport2013millimeter}, research on the feasibility of using mmWave to provide wireless communication service is presented. In \cite{akdeniz2014millimeter}, researchers investigated the mmWave propagation loss, penetration loss, interference and provided a detailed statistical mmWave channel model based on outdoor measurement. This model elaborates that mmWave system can provide an order of magnitude increase in capacity. In addition, the probabilities of outage and non-outage were demonstrated. Coverage design technology and beam-forming scheme are discussed in\cite{sun2014millimeter} \cite{roh2014millimeter}, which convince us that mmWave can be used for future wireless communication network design. Although mmWave channel can provide high capacity and low latency, it has a challenging issue: mmWave is sensitive to obstructions like buildings, trees and even nearby pedestrians or vehicles. These obstructions may cause sudden, short and severe channel downgrade. How to increase the reaction speed to the channel fluctuations and fully utilize the channel capacity is the main issue for TCP congestion control protocol design. This chapter focuses on solving this problem from an end-user perspective.

\subsection{Explicit Congestion Notification (ECN)}
\par Legacy TCP congestion control protocols consider duplicate ACKs as an indication of packet loss. With the addition of active queue management and ECN \cite{ramakrishnan2001rfc} to the network infrastructure, routers are capable of detecting congestion before queue overflows. An additional Congestion Experienced (CE) byte is added to the IP header of the packet to support this function. Routers running RED queue management can mark the CE code point with a certain probability when the average queue length is between minimal threshold and maximal threshold. When the average queue length exceeds the maximal threshold, upcoming packets' CE byte will be marked with probability $1$. The packet with marked CE will be sent to the receiver. After receiving the packet, the receiver will piggyback an ACK with marked CE to notify the sender that there is congestion in the router. No extra traffic will be generated to overload the network. The sender will reduce the congestion window to avoid queue overflow and packet loss. Using ECN to detect congestion requires the participation of routers and waiting for receiver's feedback. We develop an end-user congestion prediction scheme that utilizes the idea of ECN without involving any network devices or waiting for any feedback.
\subsection{Remy}
\par Remy \cite{winstein2013tcp} was proposed by Keith Winstein in 2013. It is a congestion-control scheme based on prior knowledge of the network and the traffic model. It is the first computer generated congestion control protocol. A RemyCC tracks only three state variables of the end-user:
\begin{itemize}
\item An exponentially weighted moving average (EWMA) of the inter-arrival time between new ACKs received (ack\_ewma).
\item An EWMA of the time between TCP sender timestamps reflected in those ACKs (send\_ewma).
\item The ratio between the most recent Round Trip Time (RTT) and the minimum RTT during the current connection (rtt\_ratio).
\end{itemize}
The algorithm does not require the support from any intermediate network devices. The end-user can adjust its sending speed following the pre-generated algorithm based on the above mentioned data. This indicates that these three variables together might reflect the congestion state of the network. 


\par Remy convinces us that designing a data-driven congestion detection algorithm using end-user data is possible. Since ECN provides accurate information of whether there is congestion in the network or not, we decide to use end-user data to predict ECN, thereby detecting the congestion. The main contribution of this chapter is developing a data-driven fast congestion detection algorithm (within one ACK inter-arrival time) using end-user data without support from receivers, routers or other network devices.

\section{Legacy TCP Performance on Emulated 5G Channel}
\label{legacy}
\par Since Ethernet has a similar capacity with mmWave channel, we use Ethernet with periodic on-off behavior to emulate mmWave channel. On period means the channel is in the state of LOS while off period means the channel is in the NLOS state. We test two existing TCP congestion control protocols: NewReno and Cubic on the emulated mmWave channel. In GENI Portal, a three-node line topology with a server, a middle box and a client was set up as in Figure \ref{genitopo}. 
\begin{figure}
\centering
\includegraphics[width=14cm]{topologyGeni.png}
\caption{GENI Topology for Performance Evaluation}
\label{genitopo}
\end{figure}
Two different kinds of channel behavior are investigated: uniformly distributed on-off period and exponentially distributed on-off period. Congestion window size (CWND) and throughput are recorded to analyze the performance as in Figure \ref{1st}, \ref{2nd}, \ref{3rd} and \ref{4th}.  The channel capacity is set to be $1$Gbps. Figure \ref{1st} illustrates the CWND of the sender when the off period is exponentially distributed with mean $10ms$, $100ms$, $1s$, while Figure \ref{2nd} shows the corresponding throughput. Figure \ref{3rd} and \ref{4th} presents the CWND and throughput when the off period is uniformly distributed between $0ms - 10ms$, $0ms - 100ms$, $0ms - 1s$. In both cases, NewReno performance is deteriorated significantly due to the high frequency of off period. Figure \ref{1st} and \ref{3rd} demonstrate that slow start will be initiated after every off period. The throughput is as low as $200$Mbps, which is $20\%$ of the full capacity. Comparing with NewReno, from Figure \ref{1st} and \ref{3rd} we conclude that Cubic is more aggressive than NewReno. When a channel recovers from off period to on period, Cubic will increase the CWND faster to utilize the channel capacity.  When channel off time is at the level of 10ms, cubic did not even notice there is a connection drop. Instead, it behaves as congestion happened in the network and reduces the CWND. Whenever the on time period is long enough as in the exponentially on-off period case, Cubic can still reach the upper limit of CWND and achieve full throughput as in Figure \ref{2nd}, while NewReno needs more time to recover. In uniformly distributed on-off period case, the channel lost occurred before Cubic fully recovered as in Figure \ref{3rd}. The CWND has a descending trend and the throughput decreases as time goes on. Based on the aforementioned observation, we conclude that even Cubic will not achieve high throughput if the channel switch between LOS (on) and NLOS (off) with a high frequency. Therefore, an aggressive congestion control protocol is necessary for the next generation networks. In the following sections, we will focus on the initial step of design this protocol: developing a fast end-user congestion prediction scheme.
\begin{figure}
\centering
\includegraphics[width=14cm]{1.eps}
\caption{CWND dynamics given exponentially On-Off channel behavior}
\label{1st}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=14cm]{2.eps}
\caption{Throughput dynamics given exponentially On-Off channel behavior}
\label{2nd}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=14cm]{3.eps}
\caption{CWND dynamics given uniformly On-Off channel behavior}
\label{3rd}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=14cm]{4.eps}
\caption{Throughput dynamics given uniformly On-Off channel behavior}
\label{4th}
\end{figure}
\section{Congestion Detection Algorithm}
\label{Design}
The design of the fast congestion detection algorithm is illustrated in detail in this section. The principle idea is to predict the congestion in network within one inter-arrival time of ACKs from end-user data. Data which can be collected from the end-user includes packet sending time, ACK arriving time and the RTT calculated from these two timestamps. We run simulations using discrete event-based Network Simulator 2 (NS2) to collect theses data for a particular network topology. 
\subsection{Network Topology}
A typical dumbbell model is presented and used in this section. The network topology is shown in Figure \ref{layout}. There are $6$ TCP traffic senders and $6$ receivers; the sender $s_{i}$ sends data to receiver $r_{i}$ through the bottleneck link $Q1\to Q2$. Both $Q1$ and $Q2$ maintain a DropTail Queue. The queue limit is set to be the bandwidth-delay product to ensure the full utilization of the link capacity.
%\begin{figure}[!htb]\centering
%   \begin{subfigure}{0.49\textwidth}
%     \frame{\includegraphics[width=6cm]{6layout.pdf}}
%     \caption{Network Topology}\label{layout}
%   \end{subfigure}
%   \begin {subfigure}{0.49\textwidth}
%     \frame{\includegraphics[width=6cm]{format.eps}}
%     \caption{NS2 TCP Trace Format}\label{NS2Format}
%   \end{subfigure}
%\end{figure}

\begin{figure}
\centering
\includegraphics[width=10cm]{6layout.pdf}
\caption{Network topology}
\label{layout}
\end{figure}
Congestion occurs when the occupied queue length exceeds the threshold $T$ on the arriving of the incoming packet. To generate traces with congestion identifiers, we mark every packet as Congestion Experienced in the queue when queue length is greater than $T$. Typical NS2 TCP traces  \cite{TraceFormat} are collected for future data processing as in Figure \ref{NS2Format}. 

\begin{figure}
\centering
\includegraphics[width=10cm]{format.eps}
\caption{NS2 TCP trace format}
\label{NS2Format}
\end{figure}
From the trace format, we can identify the packet sending time, arriving time, packet size, unique ID, etc. Matching the packet unique ID, RTT can be calculated from packet sending time and corresponding ACK receiving time. For each unique packet, if congestion happens upon its arriving, the unique ID will be recorded to be Congestion Experienced.

\par The drop tail queue behavior is displayed as follows.
% Figure \ref{queuelengthCubic}
% and Figure \ref{queuelengthNewRenoVSUDP} are two different queue behavior patterns. 
Figure \ref{queuelengthCubic} shows queue changes when traffic sources are running Cubic TCP with UDP competing traffic. 
%while Fig. \ref{queuelengthNewRenoVSUDP} shows the case one part of traffic sources are running NewReno TCP with competing UDP traffic.
In this case, when queue length is greater than $450$ packets, the incoming packets will be marked as congestion experienced.

%\begin{figure}
%\includegraphics[width=8cm]{queue.eps}
%\caption{Queue Length Dynamics (NewReno)}
%\label{queuelengthNewReno}
%\end{figure}
%\begin{figure}[!htb]\centering
%   \begin{subfigure}{0.49\textwidth}
%     \frame{\includegraphics[width=6cm]{QueueLengthCubic.eps}}
%     \caption{Queue Length (Cubic)}
%\label{queuelengthCubic}
%   \end{subfigure}
%   \begin {subfigure}{0.49\textwidth}
%     \frame{\includegraphics[width=6cm]{QueueLengthNewRenoVSUDP.eps}}
%   \caption{Queue Length (NewReno VS UDP)}
%\label{queuelengthNewRenoVSUDP}
%   \end{subfigure}
%\end{figure}

\begin{figure}
\centering
\includegraphics[width=10cm]{QueueLengthCubic.eps}
\caption{Queue length dynamics (Cubic)}
\label{queuelengthCubic}
\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=10cm]{QueueLengthNewRenoVSUDP.eps}
%\caption{Queue Length Dynamics (NewReno VS UDP)}
%\label{queuelengthNewRenoVSUDP}
%\end{figure}

\subsection{Data Collection}
Without losing of generality, we simulated several different traffic scenarios to collect end-user data. We analyze the following scenarios: 
\begin{itemize}
    \item Scenario 1: All traffic sources are running TCP NewReno with stationary bottleneck link capacity.
    \item Scenario 2: All traffic sources are running TCP Cubic with stationary bottleneck link capacity.
    % \item Scenario 3: One half traffic sources are running TCP NewReno while the other half running TCP Cubic with stationary bottle link capacity.
    \item Scenario 3: One-half of the traffic sources is running TCP NewReno while the other half is running UDP with stationary bottleneck link capacity.
    \item Scenario 4: One-half of traffic sources is running TCP Cubic while the other half is running UDP with stationary bottleneck link capacity.
    \item Scenario 5: Repeat scenario 1 and 2 for a periodically On-Off bottleneck link.
\end{itemize}
The bottleneck link for those scenarios is link $Q1 \to Q2$. Detailed simulation parameters are shown in Table \ref{tab:simuPara}.

\par Data available from end-user includes packet sending time $t_{s}$ and ACK receiving time $t_{r}$. From these two timestamps, we can calculate packet sending interval $T_{s_{i}} = t_{s_{i}} - t_{s_{i-1}}$, ACK inter-arrival time $T_{r_{i}} = t_{r_{i}} - t_{r_{i-1}}$ and Round Trip Time (RTT) $RTT_{i} = t_{s_{i}} - t_{r_{i}}$. From the above three available time intervals, we develop 5 features that are used in the congestion prediction algorithm.
\begin{table}
\begin{center}
\caption {Simulation Parameters} \label{tab:simuPara}
\begin{tabular}{ |c|c| }
 \hline
 Access Link Capacity & 1Gbps  \\
 \hline
 BottleNeck Link Capacity & 1Gbps  \\
 \hline
 Access Link Delay & 5ms  \\
 \hline
 BottleNeck Link Delay & 10ms\\
 \hline
 Queue Capacity & Bandwidth-Delay Product\\
 \hline
\end{tabular}
\end{center}
\end{table}
\begin{itemize}
\item Sending time interval between two consecutive packets $T_{s_{i}}$.
\item Receiving time interval between two consecutive ACKs $T_{r_{i}}$.
\item An exponentially-weighted moving average (EWMA) of $T_{s_{i}}$ (send\_ewma).
\item An EWMA of $T_{r_{i}}$ (ack\_ewma).
\item The ratio between the most recent RTT and the minimum RTT during the entire connection period (rtt\_ratio).
\end{itemize}

These above five features construct the feature set to train the machine learning algorithm and predict congestions.

\subsection{Congestion Prediction}
\label{CongestionPredict}
\par Every sample of the data collected from above five scenarios includes five features and a congestion identifier: Virtual ECN (0 represents non congestion experienced, 1 represents congestion experienced). The problem to be solved is a binary classification problem: train an algorithm to classify the sample data into two categories: congestion experienced and non congestion experienced. First, we apply Logistic Regression, a popular binary classification algorithm, to train the sample data and predict congestions. The area under ROC curve is $0.89$ as shown in Figure \ref{LRRoc}. The performance of Logistic Regression is good ($0.80 - 0.90$). The decision boundary is $ -2.32*f_{1} - 33.36*f_{2} - 12.65*f_{3} - 15.30*f_{4} + 2.03*f_{5} - 2.69 = 0$, where $f_{1}, f_{2}, f_{3}, f_{4}, f_{5}$ are the five corresponding features.
\begin{figure}
\centering
\includegraphics[width=10cm]{LRRoc.png}
\caption{ROC curve of Logistic Regression classifier}
\label{LRRoc}
\end{figure}




\par Next, we applied Decision Tree algorithm and regulated the maximum depth of the tree to be $5$ and the minimum samples in each leaf node to be $10000$ to avoid overfitting. Figure \ref{DecisionTreeRoc} presents the ROC curve of Decision Tree Classifier. The area under ROC curve is $0.98$. This indicates excellent (area under ROC: $0.90 - 1$) prediction result. The feature importance is given as [$0.011703$, $0.00085$, $0.000000$, $0.03985$, $0.94760$] for [$T_{s_{i}}$, $T_{r_{i}}$, send\_ewma, receive\_ewma, rtt\_ratio], from which we can conclude that rtt\_ratio is the most important feature to predict congestions. 
%\begin{figure}
%\centering
%\includegraphics[width=10cm]{DecisionTreeRoc.png}
%\caption{ROC curve of Decision Tree Classifier (max\_depth = 5, min\_samples\_leaf = 10000)}
%\label{DecisionTreeRoc}
%\end{figure}

\begin{figure}[!htb]\centering
   \begin{subfigure}{0.49\textwidth}
     \includegraphics[width=6cm]{DecisionTreeRoc.png}
\caption{max\_depth = 5, min\_samples\_leaf = 10000}
\label{DecisionTreeRoc}

   \end{subfigure}
   \begin {subfigure}{0.49\textwidth}
     \includegraphics[width=6cm]{DTRoc.png}
\caption{max\_depth = 3, min\_samples\_leaf = 100000}
\label{DTRoc}
   \end{subfigure}
   \caption{ROC curve of Decision Tree classifier}
\label{fig:roc1}
\end{figure}
Furthermore, we changed the maximum depth to $3$ and minimum samples of each leaf node to $100,000$, the ROC is displayed in Figure \ref{DTRoc}. The feature importance turns to [$0$, $0$, $0$, $0$, $1$]. This indicates the decision tree is doing the binary classification merely based on rtt\_ratio. All other features do not contribute to the binary classification process at all. 
%\begin{figure}
%\centering
%\includegraphics[width=10cm]{DTRoc.png}
%\caption{ROC curve of Decision Tree Classifier (max\_depth = 3, min\_samples\_leaf = 100000)}
%\label{DTRoc}
%\end{figure}
The decision tree is shown in Figure \ref{DT}. In this figure, the first layer is classified by X[4] which is rtt\_ratio. The cumulative density function (CDF) of rtt\_ratio is given in Figure \ref{CDFrtt}. The blue line is the CDF curve of rtt\_ratio when congestion is experienced while the red line is the CDF curve when congestion is not experienced. Take rtt\_ratio $=1.2$ as an example, the probability of rtt\_ratio to be less than $1.13$ is $82\%$ when there is no congestion. However, this probability is approximately $0\%$ when congestion experienced. This is consistent with the result of feature importance generated from Decision Tree algorithm. 

\begin{figure}
\centering
  \includegraphics[width=\textwidth,height=4cm]{tree.png}
  \caption{Decision Tree (max\_depth = 3, min\_samples\_leaf = 100000)}
  \label{DT}
\end{figure}

\begin{figure}[!htb]\centering
   \begin{subfigure}{0.49\textwidth}
     \frame{\includegraphics[width=6cm]{cdfrtt1.eps}}
\caption{CDF of rtt\_ratio with same propagation delay}
\label{CDFrtt}

   \end{subfigure}
   \begin {subfigure}{0.49\textwidth}
     \frame{\includegraphics[width=6cm]{cdfrtt2.eps}}
\caption{CDF of rtt\_ratio with different propagation delay}
\label{CDFrttDiff}
   \end{subfigure}
\caption{CDF of rtt\_ratio}
\label{fig:cdf1}
\end{figure}



%\begin{figure}
%\centering
%\includegraphics[width=10cm]{cdfrtt1.eps}
%\caption{CDF of rtt\_ratio}
%\label{CDFrtt}
%\end{figure}

\par The above results are generated from a network with fixed propagation delay. To learn how this prediction algorithm works when network condition changes, we investigate Scenario 1 and 2 with variations of propagation delay. We simulate networks with access delay and queue capacity as in Table \ref{tab:varyRTT}.
\begin{table}
\begin{center}
\caption {Simulation Parameters} \label{tab:varyRTT}
\begin{tabular}{ |c|c| }
 \hline
 Access Link Delay & 5ms, 10ms, 20ms  \\
 \hline
 BottleNeck Link Delay & 10ms, 20ms, 40ms\\
 \hline
 Queue Capacity & Bandwidth-Delay Product\\
 \hline
\end{tabular}
\end{center}
\end{table}
The Logistic Regression algorithm generates ROC curve which is shown in Figure \subref{LRROCDiff}. The area under ROC curve is $0.65$, which indicates poor performance (area under ROC curve: $0.60-0.70$). The decision boundary is $ -0.47*f_{1} - 7.33*f_{2} - 0.35
*f_{3} - 1.41*f_{4} + 5.20*f_{5} - 5.54 = 0$. The coefficient of each feature follows the same pattern as before. Decision Tree classifier is then applied to the collected dataset. The ROC curve is shown in Figure. \subref{DTROCDiff}. The area under ROC curve is $0.70$, which is on the boundary of poor performance. The feature importance is [$0.000955787581$, $0.000389816495$, $0.00489484823$, $0.0636899225$, $0.930069625$]. This is consistent with the previous result that rtt\_ratio is the most important feature. Those two figures indicate that when network condition changes, the predictability of congestion from the aforementioned five features decreases. Figure. \ref{CDFrttDiff} shows the CDF of rtt\_ratio with different link propagation delay. The figure indicates near $30\%$ overlap between CDF curves of Congestion Experience packet and No Congestion packet. This again verifies the prediction results.
\begin{figure}[!htb]\centering
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=7cm]{LRRocDiffProp.png}
\caption{ROC curve of Logistic Regression classifier with different propagation delay}
\label{LRROCDiff}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\includegraphics[width=7cm]{DTRocDiffProp.png}
\caption{ROC curve of Decision Tree classifier with different propagation delay}
\label{DTROCDiff}
\end{subfigure}
\caption{ROC curves with different propagation delay}
\label{fig:roc2}
\end{figure}
%\begin{figure}
%\centering
%\includegraphics[width=10cm]{cdfrtt2.eps}
%\caption{CDF of rtt\_ratio with Different Propagation Delay}
%\label{CDFrttDiff}
%\end{figure}




\section{Chapter Summary}
\label{Conclusions and Future Work}
In this chapter, we proposed a data-driven machine learning congestion detection algorithm. Datasets are collected using NS2 for a dumbbell model with five different traffic scenarios. Five features are formatted from end-user data. When the network condition is not changing, which means the propagation delay of each link is stable, our algorithm can detect congestion with high precision from those five features. In contrast, if the network condition changes, the algorithm fails to work. In both cases, rtt\_ratio is the most important feature to predict congestion. The other four features are less useful when doing the binary classification. CDF of rtt\_ratio explains our conclusion. For stable network, a fast reacting congestion control algorithm can be designed based on our congestion detection algorithm for mmWave communication networks.
